---
tags: ["TP"]
aliases: ["TP3", "TP3 sur la rÃ©plication physique"]
---

# La rÃ©plication physique d'un cluster Postgres
---

> Eldar Kasmamytov p1712650
> (Je suis en monÃ´me)

<br/>

## I. La rÃ©plication
---

> ðŸ¤” <span style="color: #8357e9; font-weight: bold;">Question:</span> Quâ€™est-ce quâ€™une rÃ©plication en WAL shipping ?  
> ðŸ’¡ <span style="color: #8357e9; font-weight: bold;">RÃ©ponse:</span> C'est une solution au problÃ¨me de la rÃ©plication qui consiste Ã  surveiller le serveur principale pour tout changement dans la base de donnÃ©es au travers les WALs. Elle nÃ©cÃ©ssite une architecture multi-nÅ“uds dans laquelle chaque nÅ“ud est attribuÃ© d'un rÃ´le :  
>  
> - Un seul nÅ“ud **Queen** (appelÃ© Ã©galement *read-write*, *master* ou *primary*)  
>   C'est le seul nÅ“ud pouvant accepter des requÃªtes en lÃ©cture **et** en Ã©criture (*read-write*). Les transactions sont ensuite **rÃ©pliquÃ©es** sur d'autres nÅ“uds en leur fournissant les fichiers **WAL** de la Queen, d'oÃ¹ le nom de ce mÃ©chanisme "WAL *Shipping*".  
>   <br/>
>     - Ces nÅ“uds recevant les WAL de la Queen (et ainsi rÃ©pliquant les transactions) sont appelÃ©s **Princess** (ou aussi *standby*, *secondary*). En cas de panne de la Queen, une des Princess prend le relais et devient une nouvelle Queen.  

Il existe diffÃ©rentes approches pour la rÃ©plication, notamment *File-Based* et *Streaming*. Dans la suite, nous mettrons en place une rÃ©plication *Streaming*.  

- **Les buts de cette partie :**  
  - Mettre en place une rÃ©plication streaming  
  - Modifier la rÃ©plication pour quâ€™elle devienne synchrone  

### 1.1 RÃ©plication en mode "Streaming"

> ðŸ¤” <span style="color: #8357e9; font-weight: bold;">Question:</span> Quâ€™est-ce quâ€™une rÃ©plication en mode â€œstreamingâ€ ?  
> ðŸ’¡ <span style="color: #8357e9; font-weight: bold;">RÃ©ponse:</span> C'est une technique de rÃ©plication permettant au serveur primaire (ou **Queen**) de diffuser/stream en direct les entrÃ©es de WAL aux serveurs secondaires (ou **Princess**) au moment quand elles sont gÃ©nÃ©rÃ©es, sans attendre que le fichier WAL (un segment de 16MB) soit rempli en entier.  
> [Lien vers la documentation officielle](https://www.postgresql.org/docs/15/warm-standby.html#STREAMING-REPLICATION)  

> ðŸ¤” <span style="color: #8357e9; font-weight: bold;">Question:</span> Quelle est la diffÃ©rence entre une rÃ©plication logique et une rÃ©plication physique ?  
> ðŸ’¡ <span style="color: #8357e9; font-weight: bold;">RÃ©ponse:</span> La rÃ©plication logique fonctionne en modÃ¨le qui peut Ãªtre dÃ©crit comme *Publish / Subscribe*. Elle rÃ©plique les modifications des **objets** de base de donnÃ©es (tels que les tables) effectuÃ©es sur un serveur primaire (*Publisher*) vers un ou plusieurs serveurs qui lui sont abonnÃ©s (*Subscribers*). Les donnÃ©es rÃ©pliquÃ©es peuvent Ãªtre sÃ©lectionnÃ©es et mÃªme eventuellement traitÃ©es/transformÃ©es avant l'envoi. Cela permet une flexibilitÃ© et un plus petit volume de donnÃ©es transmis comparÃ© Ã  la rÃ©plication physique. La rÃ©plication logique permet Ã©galement de rÃ©pliquer les donnÃ©es sur une diffÃ©rente plateforme et/ou version de PostgreSQL, ce qui n'est pas possible avec la rÃ©plication physique.  
> 
> Tandis que la rÃ©plication physique rÃ©plique tous les **fichiers physiques** qui constituent un cluster PostgreSQL, y compris les fichiers WAL. La mise en place et le fonctionnement de la rÃ©plication physique est plus simple comparÃ© Ã  celle de la rÃ©plication logique. Elle est aussi plus performante, car fonctionne au niveau des blocks.  

#### 1.1.1 Mise en place de la rÃ©plication en mode Streaming

Nous allons mettre en place une rÃ©plication avec une seule Princess.  
Cf. [La documentation officielle sur la rÃ©plication en WAL Shipping de Postgres](https://www.postgresql.org/docs/15/warm-standby.html)

##### Configurer l'authentification

- Sur la **Queen**, crÃ©er un rÃ´le dÃ©diÃ© pour la rÃ©plication avec les privilÃ¨ges "`REPLICATION`" et "`LOGIN`" :  

  > [`CREATE USER`](https://www.postgresql.org/docs/current/role-attributes.html) est Ã©quivalent Ã  `CREATE ROLE` sauf qu'il inclut le privilÃ¨ge "`LOGIN`" par dÃ©faut.  

  ```bash
  sudo su postgres
  psql -c "create user replicator password 'jw8s0F4' replication;"
  ```

  Vous verrez un message de succÃ¨s comme ceci si tout se passe bien :  

  ```log
  CREATE ROLE
  ```

- Toujours sur la **Queen**, ajouter une entrÃ©e dans le fichier `pg_hba.conf` afin de permettre Ã  la Princess de se connecter en tant qu'utilisateur `replicator` Ã  la *pseudo* base de donÃ©es `replication` :  

  ```bash
  sudo -u postgres sh -c 'echo \
    "host    replication     replicator      192.168.75.231/32           md5" \
    >> /etc/postgresql/15/main/pg_hba.conf'
  ```

  Il est Ã©galement nÃ©cÃ©ssaire d'ajouter l'adresse IP de la Princess dans le paramÃ¨tre [`listen_addresses`](https://www.postgresql.org/docs/15/runtime-config-connection.html#GUC-LISTEN-ADDRESSES) de la Queen (par dÃ©faut, la valeur est `localhost`). Dans ce TP, nous autoriserons toutes les connexions, cÃ d `*`. Modifier la configuration `postgresql.conf` :  

  - Connections and Authentication :  
    - Connection Settings :  
      - `listen_addresses = '*'`

  RedÃ©marrer le cluster Postgres pour que les changements soient pris en compte :  
  ```bash
  sudo pg_ctlcluster 15 main reload
  ```

- Sur la **Princess**, nous allons modifier le paramÃ¨tre [`primary_conninfo`](https://www.postgresql.org/docs/15/runtime-config-replication.html#GUC-PRIMARY-CONNINFO) dans la configuration `postgresql.conf` :  

  - REPLICATION :  
    - Standby Servers :  
      - `primary_conninfo = 'host=192.168.75.149 port=5432 user=replicator'`

  Le mot de passe de `replicator` sera mis dans le fichier `~/.pgpass` :  

  ```bash
  echo "192.168.75.149:*:replication:replicator:jw8s0F4" >> ~/.pgpass
  chmod 600 ~/.pgpass
  ```

##### Mettre en place un repository partagÃ©

Avant de commencer la rÃ©plication, il nous faut rÃ©cupÃ©rer une des sauvegardes du serveur primaire (Queen) sur le secondaire (Princess).  Pour cela, nous allons mettre en place un rÃ©pertoire de sauvegardes partagÃ©, qui sera accÃ©ssible depuis le secondaire Ã  travers une connexion NFS.  

Installer `nfs` sur les 2 machines. Pour faire simple, on installera le client et le serveur :  
```bash
sudo apt update
sudo apt install nfs-kernel-server
```

- Sur le serveur primaire (Queen) :  

  Ajouter une ligne au fichier `/etc/exports` :  
  ```bash
  sudo sh -c 'echo "/var/lib/pgbackrest    192.168.75.231(rw,sync,no_subtree_check,no_root_squash)" >> /etc/exports'
  ```

  Exporter :  
  ```bash
  sudo exportfs -a
  ```

  RedÃ©marrer le service `nfs` :  
  ```bash
  sudo systemctl restart nfs-kernel-server
  ```

- Sur le serveur secondaire (Princess) :  

  CrÃ©er un dossier dans lequel on va mettre nos backups :  
  ```bash
  sudo mkdir -p /mnt/nfs/pgbackrest
  ```

  Monter le dossier distant avec les paramÃ¨tres nÃ©cÃ©ssaires :  
  ```bash
  sudo mount -o rw,hard,intr,noatime,nolock,nocto 192.168.75.149:/var/lib/pgbackrest /mnt/nfs/pgbackrest
  ```

##### Configurer et lancer `pgBackRest` sur le serveur secondaire

>â•**Remarque:** Il est Ã©galement possible de faire un *base backup* avec l'outil [`pg_basebackup`](https://www.postgresql.org/docs/15/continuous-archiving.html#BACKUP-BASE-BACKUP)  

Changer le chemin vers le rÃ©pertoire de sauvegardes :  
```conf
[global]
repo1-path=/mnt/nfs/pgbackrest
```

VÃ©rifier que les sauvegardes sont bien visibles par `pgBackRest` :  
```bash
sudo -iu postgres pgbackrest --stanza=main info
```

Lancer une restauration des backups :  
```bash
sudo pg_ctlcluster 15 main stop
sudo -u postgres pgbackrest --stanza=main --delta --type=standby restore
```

##### DÃ©marrer le serveur secondaire en mode `standby`

Il est maintenant possible de crÃ©er la rÃ©plication sur la **Princess**. Pour ce faire, comme indiquÃ© sur [la page officielle](https://www.postgresql.org/docs/15/warm-standby.html#STANDBY-SERVER-OPERATION), nous allons crÃ©er un fichier vide "`standby.signal`" dans le rÃ©pertoire de donnÃ©es du cluster Postgres (ici, "`/var/lib/postgresql/15/main/`"), qui servira de *signal* Ã  Postgres lors de son dÃ©marrage pour activer le mode "`standby`" (la rÃ©plication en Streaming).  

> Ce fichier est crÃ©Ã© automatiquement si l'option `--type=standby` a Ã©tÃ© spÃ©cifiÃ©e dans la commande `restore` de `pgBackRest` !  

Sur la Princess :  

```bash
sudo su postgres
touch /var/lib/postgresql/15/main/standby.signal
```

RedÃ©marrer le cluster Postgres :  

```
sudo pg_ctlcluster 15 main restart
```

Apres le redÃ©marrage, le cluster doit Ãªtre en mode `standby`.  

#### 1.1.2 VÃ©rification de la rÃ©plication

Pour vÃ©rifier que la Princess est bien en mode `standby`, on peut afficher les logs de Postgres :  

```bash
sudo -u postgres cat /var/log/postgresql/postgresql-15-main.log
```

Le fichier `standby.signal` est bien pris en compte si vous voyez cette ligne dans les logs :  

```log
LOG:  entering standby mode
```

La rÃ©plication Streaming est bien mise en place si vous voyez un message comme ceci :  

```log
LOG:  started streaming WAL from primary at 0/F000000 on timeline 5
```

> Si vous voyez un message d'erreur dÃ» Ã  la connexion Ã©chouÃ©e :  
> ```log
> FATAL:  could not connect to the primary server
> ```
> VÃ©rifiez que l'adresse IP de la Princess est bien inclu dans le paramÃ¨tre `listen_addresses` de la Queen.  

> Si vous voyez ce message d'erreur :  
> ```log
> FATAL:  database system identifier differs between the primary and standby
> ```
> Veuillez vous assurer que le cluster Postgres sur la Princess a Ã©tÃ© restorÃ© avec une sauvegarde de la Queen.  

<br/>

### 1.2 RÃ©plication Synchrone

> ðŸ¤” <span style="color: #8357e9; font-weight: bold;">Question:</span> Une rÃ©plication synchrone peut-elle fonctionner en WAL shipping ?  
> ðŸ’¡ <span style="color: #8357e9; font-weight: bold;">RÃ©ponse:</span> D'aprÃ¨s la [documentation officielle](https://www.postgresql.org/docs/15/warm-standby.html#SYNCHRONOUS-REPLICATION), oui.  

> ðŸ¤” <span style="color: #8357e9; font-weight: bold;">Question:</span> Quels sont les inconvÃ©nients dâ€™une rÃ©plication synchrone ?  
> ðŸ’¡ <span style="color: #8357e9; font-weight: bold;">RÃ©ponse:</span> Elle peut relentir le temps de rÃ©ponse pour les requÃªtes, car une transaction doit attendre le retour du nombre minimum de serveurs *standby* synchrones (indiquÃ© dans le paramÃ¨tre `synchronous_standby_names`). En plus, la rÃ©plication synchrone est une contrainte supplÃ©mentaire lors de la configuration de la [haute disponibilitÃ©](https://www.postgresql.org/docs/15/warm-standby.html#SYNCHRONOUS-REPLICATION-HA), car on doit maintenir le minimum de serveurs *standby* en Ã©tat opÃ©rationnel.  

[Lien](https://www.postgresql.org/docs/15/warm-standby.html#SYNCHRONOUS-REPLICATION) vers la documentation officielle en ligne.  

#### 1.2.1 Configuration

Une fois la rÃ©plication streaming est mise en place, on peut la rendre synchrone en mettant Ã  jour la configuration (le paramÃ¨tre [`synchronous_standby_names`](https://www.postgresql.org/docs/current/runtime-config-replication.html#GUC-SYNCHRONOUS-STANDBY-NAMES)) sur le serveur primaire (Queen) :  

  - REPLICATION :  
    - Primary Server :  
      - `synchronous_standby_names = '1 (standby1)'`

Le premier nombre (ici, `1`) correspond au nombre de serveurs `standby` synchrones dont le retour est nÃ©cÃ©ssaire afin de *commit* les transactions.  

Les noms des serveurs `standby` dans les paranthÃ¨ses doivent correspondre au paramÃ¨tre `application_name` dans le fichier `postgresql.conf` (ou "`*`" peut Ãªtre utilisÃ© pour *matcher* tout standby) :  

  - REPLICATION :  
    - Standby Servers :  
      - `application_name = 'standby1'`

Ou, vu que nous utilisons `pgBackRest`, le nom de notre standby peut Ãªtre indiquÃ© dans la configuration de `pgBackRest` :  

```conf
[main]
recovery-option=application_name=standby1
```

> Cette configuration ne s'applique qu'au cas d'un seul serveur `standby`. Si vous en avez plus, veuillez lire la [section](https://www.postgresql.org/docs/current/warm-standby.html#SYNCHRONOUS-REPLICATION-MULTIPLE-STANDBYS) correspondante de la documentation.  

AprÃ¨s avoir mis Ã  jour la configuration, redÃ©marrer le cluster :  

```bash
sudo pg_ctlcluster 15 main restart
```

<br/>

## II. VÃ©rification
---

**Le but :** S'assurer que la rÃ©plication fonctionne correctement  

Nous allons essayer chacune des techniques suivantes :  

### 2.1 Regarder lâ€™Ã©tat de la rÃ©plication dans uneÂ [vue systÃ¨me](https://www.postgresql.org/docs/current/monitoring-stats.html#MONITORING-PG-STAT-REPLICATION-VIEW)

Il est possible de vÃ©rifier la configuration en affichant le contenu de la vue `pg_stat_replication` :  

Se connecter Ã  la base `postgres` en tant que `postgres` :  
```bash
sudo su postgres
psql
```

ExÃ©cuter la requÃªte suivante :  
```SQL
SELECT * FROM pg_stat_replication;
```

Ce qui nous intÃ©resse dans cette vue c'est les colonnes `state`, `sync_state` pour nos standbys :  

| ... | state | sync_state |
| --- | --- | --- |
| ... | streaming | sync |

<br/>

### 2.2 Modifier des donnÃ©es sur la reine et vÃ©rifier la modification de ces donnÃ©es sur la princesse

Sur les deux machines, se connecter Ã  la base `benchdb` en tant que `postgres` :  
```bash
sudo -u postgres psql -d benchdb
```

VÃ©rifier le nombre de lignes avant la suppression :  
```sql
SELECT COUNT(*) FROM pgbench_accounts WHERE bid=2;
```

```log
 count
--------
 100000
(1 row)
```

Sur la Queen, supprimer les lignes deÂ `pgbench_accounts`Â pour lesquelles la colonneÂ `bid`Â vaut 2 :  
```sql
DELETE FROM pgbench_accounts WHERE bid=2;
```

Sur la Princess, vÃ©rifier le nombre de lignes aprÃ©s la suppression :  
```sql
SELECT COUNT(*) FROM pgbench_accounts WHERE bid=2;
```

```log
 count
-------
     0
(1 row)
```

<br/>

### 2.3 Changer de fichier WAL courant et vÃ©rifier le fichier WAL courant sur la princesse

Nous allons utiliser la fonction d'administration [`pg_switch_wal`](https://www.postgresql.org/docs/current/functions-admin.html#FUNCTIONS-ADMIN-BACKUP) en tant que `postgres` afin de changer de fichier WAL :  

Sur la Queen, se connecter en tant que `postgres` :  
```bash
sudo -u postgres psql
```

Appeler la fonction :  
```sql
SELECT pg_switch_wal();
```

```log
 pg_switch_wal
---------------
 0/13208148
(1 row)
```

Sur la Princess, vÃ©rifier le fichier WAL courant :  
```bash
sudo -u postgres psql -c "SELECT pg_current_wal_lsn();"
```

<br/>

### 2.4 Pour la rÃ©plication synchrone, arrÃªter la princesse et essayer dâ€™Ã©crire sur la reine

ArrÃªter la Princess :  
```bash
sudo pg_ctlcluster 15 main stop
```

Faire une modification sur la Queen :  
```bash
sudo -u postgres psql -d benchdb -c "DELETE FROM pgbench_accounts WHERE bid=4;"
```

Cette transaction [ne sera pas *commit*](https://www.postgresql.org/docs/15/warm-standby.html#SYNCHRONOUS-REPLICATION-HA), car la Queen doit attendre la rÃ©ponse du minimum de standbys (ici, `1`, comme configurÃ© plus haut) et vu que la Princess est arrÃªtÃ©e, la Queen ne recevra jamais sa rÃ©ponse.  

Pour que la transaction soit *commit*, nous devons dÃ©marrer la Princess.  

